process PublishResults {
    label 'python_tools'

    publishDir "${params.out_dir}/results", mode: 'copy', pattern: "all_designs.csv"
    publishDir "${params.out_dir}/results", mode: 'copy', pattern: "success_metrics.json"
    publishDir "${params.out_dir}/results", mode: 'copy', pattern: "best_designs*"
    publishDir "${params.out_dir}/results", mode: 'copy', pattern: "ranked_designs*"
    publishDir "${params.out_dir}/run/publish", mode: 'copy', pattern: "filter_best_designs.log"
    publishDir "${params.out_dir}/run/publish", mode: 'copy', pattern: "rank_designs.log"

    input:
    path final_pdbs
    path csv_scores
    val fold_count
    val filter_fold_count
    val seq_count
    val filter_seq_count
    val pred_count
    val filter_pred_count
    val filter_analysis_count

    output:
    path "all_designs.csv"
    path ("best_designs*"), optional: true
    path ("ranked_designs*"), optional: true
    path "filter_best_designs.log"
    path ("rank_designs.log"), optional: true
    path "success_metrics.json"

    script:

    println("\nPipeline results summary:")
    if (!params.skip_fold & !params.skip_fold_seq & !params.skip_fold_seq_pred){
        if(params.design_mode == 'bindcraft_denovo'){
            println("* Fold designs generated by BindCraft: ${fold_count}")
        }else{
            println("* Fold designs generated by RFdiffusion: ${fold_count}")
        }
        println("* Fold designs after filtering: ${filter_fold_count}")
    }
    if (!params.run_fold_only){
        if (!params.skip_fold_seq & !params.skip_fold_seq_pred){
            println("* Sequence designs generated by ${params.seq_method.toUpperCase()} (Folds * ${params.seqs_per_design}): ${seq_count} ")
            println("* Sequence designs after ${params.seq_method.toUpperCase()} filtering: ${filter_seq_count}")
        }
        if (!params.skip_fold_seq_pred){
            println("* Predictions generated by ${params.pred_method.toUpperCase()}: ${pred_count}")
            println("* Predictions after ${params.pred_method.toUpperCase()} filtering: ${filter_pred_count}")
        }
    }
    def final_pdbs_exist = final_pdbs.name != "placeholder.pdb"
    def param_combo = params.bindsweeper_param_combo ?: "default"
    def num_processes = task.cpus - 1
    
    // Use user-specified ranking metric or default based on prediction method
    def ranking_metric = params.ranking_metric ?: (params.pred_method == 'boltz' ? 'boltz_ipSAE_min' : 'af2_pae_interaction')
    
    // Check for placeholder
    if (final_pdbs_exist) {
        if (!params.run_fold_only){
            println("* Final predictions after Analysis filtering: ${filter_analysis_count}")
            if (params.rank_designs & (params.max_designs != null)){
                println("* After Ranking, will output the best ${params.max_designs} designs\n")
            } else {
                println("* After Ranking, will output all ${filter_analysis_count} designs\n")
            }
        }
        """
        # Create best_designs.csv with only rows for final PDB files
        python /scripts/filter_best_designs.py \
            --csv ${csv_scores} \
            --pdb-dir ./ \
            --output-csv best_designs.csv \
            --output-dir best_designs \
            2>&1 | tee filter_best_designs.log

        # Generate success metrics JSON
        python /scripts/generate_success_metrics.py \
            --fold-count ${fold_count} \
            --filter-fold-count ${filter_fold_count} \
            --seq-count ${seq_count} \
            --filter-seq-count ${filter_seq_count} \
            --pred-count ${pred_count} \
            --filter-pred-count ${filter_pred_count} \
            --filter-analysis-count ${filter_analysis_count} \
            --final-designs-count ${filter_analysis_count} \
            --parameter-combination "${param_combo}" \
            --output success_metrics.json
            
        # Optionally rank designs
        if [ ${params.rank_designs} == 'true' ] ; then
            echo "Ranking designs by ${ranking_metric}"
            python /scripts/rank_designs.py \
                --csv best_designs.csv \
                --pdb-dir best_designs \
                --output-csv ranked_designs.csv \
                --output-dir ranked_designs \
                --ranking-metric ${ranking_metric} \
                ${params.max_designs ? "--max-designs ${params.max_designs}" : ""} \
                ${params.max_seqs_per_fold ? "--max-seqs-per-fold ${params.max_seqs_per_fold}" : ""} \
                2>&1 | tee rank_designs.log
            
            # Optionally compress ranked PDB files
            if [ ${params.zip_pdbs} == 'true' ] ; then
                tar -h \
                    --use-compress-program="pigz -p ${num_processes}" \
                    -cf ranked_designs.tar.gz \
                    ranked_designs
                rm -rf ranked_designs
            fi
        fi
        
        # Optionally compress best_designs PDB files
        if [ ${params.zip_pdbs} == 'true' ] ; then
            tar -h \
                --use-compress-program="pigz -p ${num_processes}" \
                -cf best_designs.tar.gz \
                best_designs
            rm -rf best_designs
        fi
        """
    }
    else {
        println("No designs survived filtering\n")
        """
        echo "No designs survived filtering" > filter_best_designs.log
        
        # Generate success metrics JSON even for failed runs
        python /scripts/generate_success_metrics.py \
            --fold-count ${fold_count} \
            --filter-fold-count ${filter_fold_count} \
            --seq-count ${seq_count} \
            --filter-seq-count ${filter_seq_count} \
            --pred-count ${pred_count} \
            --filter-pred-count ${filter_pred_count} \
            --filter-analysis-count ${filter_analysis_count} \
            --final-designs-count 0 \
            --parameter-combination "${param_combo}" \
            --output success_metrics.json
        """
    }
}
